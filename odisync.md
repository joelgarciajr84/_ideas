 Hist√≥ria 1: Compara√ß√£o de arquivos JSON entre dias consecutivos usando AWS Glue
Narrativa / Vis√£o do usu√°rio
Como engenheiro de dados, quero um Glue Job que compare arquivos JSON dos dias atual e anterior em dois buckets, para identificar diferen√ßas e salv√°-las em um bucket espec√≠fico de "diffs".

DOR
Buckets de entrada e sa√≠da definidos.

Glue habilitado na conta.

Estrutura dos arquivos JSON acordada.

Descri√ß√£o
Um job do AWS Glue ser√° executado diariamente, lendo arquivos JSON de dois buckets (dia atual e dia anterior), comparando seus conte√∫dos e salvando as diferen√ßas encontradas em um terceiro bucket, o bucket_diffs.

Premissas de desenvolvimento
Os arquivos possuem uma chave √∫nica que permite a compara√ß√£o.

A estrutura dos arquivos permanece consistente.

A data do processamento ser√° passada como par√¢metro (ex: "hoje").

Informa√ß√µes t√©cnicas
Linguagem: PySpark no Glue.

Buckets:

bucket_ontem/ano/mes/dia/

bucket_hoje/ano/mes/dia/

bucket_diffs/ano/mes/dia/

Output: arquivos JSON com diferen√ßas.

IAM Role do Glue com permiss√µes de leitura e escrita nos 3 buckets.

Passo a passo
Criar buckets S3 (ontem, hoje, diffs).

Criar IAM Role com permiss√µes nos buckets.

Desenvolver script PySpark de compara√ß√£o.

Criar Glue Job com par√¢metros de data.

Criar trigger di√°ria via Glue ou EventBridge.

Crit√©rio de aceite (DOD)
Glue Job criado e vis√≠vel no console.

Arquivos de diffs gerados corretamente.

Job executa com par√¢metros de data e salva sa√≠da no bucket correto.

Permiss√µes validadas com sucesso.

üîπ Hist√≥ria 2: Disparo de eventos SQS a partir de arquivos de diff salvos no S3
Narrativa / Vis√£o do usu√°rio
Como engenheiro de plataforma, quero que cada novo arquivo de diffs salvo no bucket envie uma mensagem a uma fila SQS, para que outras aplica√ß√µes possam reagir a essas mudan√ßas.

DOR
Bucket de diffs j√° criado.

Permiss√µes de eventos S3 para SQS dispon√≠veis.

Descri√ß√£o
A cada novo arquivo JSON salvo no bucket de diffs, uma mensagem √© enviada automaticamente para uma fila SQS com metadados b√°sicos (nome do arquivo, data, etc).

Premissas de desenvolvimento
Os eventos ser√£o apenas de "PUT".

A fila ser√° do tipo standard.

Mensagens ter√£o formato simples e padronizado.

Informa√ß√µes t√©cnicas
Fila SQS: diffs-events-queue

Bucket: bucket_diffs

Notifica√ß√£o: evento PUT ‚Üí envia mensagem com caminho do arquivo.

IAM policy para permitir s3:PutObject ‚Üí sqs:SendMessage.

Passo a passo
Criar fila SQS com policy de acesso S3.

Adicionar notifica√ß√£o no bucket_diffs.

Validar envio autom√°tico ao subir arquivos de teste.

Crit√©rio de aceite (DOD)
Arquivos novos no bucket geram mensagens na fila.

Mensagens seguem padr√£o { path: "...", date: "..." }.

Fila vis√≠vel e monitor√°vel no console AWS.

IAM policies auditadas e seguras.

üîπ Hist√≥ria 3: ECS Task que consome fila SQS e envia eventos para Kafka
Narrativa / Vis√£o do usu√°rio
Como engenheiro de dados, quero um ECS agendado que consome eventos da fila SQS de diffs e envie os dados para um t√≥pico Kafka, para alimentar fluxos downstream.

DOR
Fila SQS configurada.

Kafka acess√≠vel (p√∫blico ou via VPC).

Defini√ß√£o do payload Kafka.

Descri√ß√£o
Uma task do ECS ser√° executada diariamente (ou sob demanda) para ler mensagens da fila SQS, processar os arquivos referenciados no S3, extrair dados e publicar em um t√≥pico Kafka.

Premissas de desenvolvimento
Cada mensagem na fila cont√©m o caminho de um arquivo v√°lido no bucket de diffs.

O ECS possui permiss√£o para acessar o S3 e Kafka.

O formato da mensagem Kafka j√° √© conhecido.

Informa√ß√µes t√©cnicas
Servi√ßo: ECS Fargate com task agendada via EventBridge.

Docker container custom com c√≥digo Python (ou outro) para:

Ler mensagens da fila SQS.

Baixar e processar os arquivos no S3.

Enviar o conte√∫do para o t√≥pico Kafka.

Kafka configur√°vel por vari√°veis de ambiente ou Secrets Manager.

Logs via CloudWatch.

Passo a passo
Criar container com app consumidor da SQS e publisher Kafka.

Criar IAM Role com acesso a S3, SQS, Kafka.

Criar cluster ECS e task definition.

Criar agendamento di√°rio √†s 2h via EventBridge.

Configurar CloudWatch Logs para monitoramento.

Crit√©rio de aceite (DOD)
Task ECS executa com sucesso e sem erros.

Mensagens da fila SQS s√£o processadas e arquivos baixados.

Eventos s√£o publicados no t√≥pico Kafka com sucesso.

Logs vis√≠veis no CloudWatch.

Task pode ser monitorada e reagendada facilmente.

